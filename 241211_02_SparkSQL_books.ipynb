{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69254af5-4c0e-46d2-817d-185ad04a215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 10:29:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"241211_01_SparkSQL_SQLtest\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b540ad1-2eb6-44c3-b0d9-9cb995e63b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "user_data = [\n",
    "    Row(user_id=1, username=\"A\", address=\"서울\"),\n",
    "    Row(user_id=2, username=\"B\", address=\"대전\"),\n",
    "    Row(user_id=3, username=\"C\", address=\"경기도\"),\n",
    "    Row(user_id=4, username=\"D\", address=None),\n",
    "    Row(user_id=5, username=\"E\", address=None),\n",
    "    Row(user_id=6, username=\"F\", address=\"부산\"),\n",
    "    Row(user_id=7, username=\"G\", address=\"대구\"),\n",
    "    Row(user_id=8, username=\"H\", address=\"광주\"),\n",
    "    Row(user_id=9, username=\"I\", address=\"울산\"),\n",
    "    Row(user_id=10, username=\"J\", address=\"강원도\"),\n",
    "    Row(user_id=11, username=\"K\", address=\"충청도\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1cde1b3-35d7-4b01-b9cd-58d780f7fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = spark.createDataFrame(user_data)\n",
    "user_df.createOrReplaceTempView('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b51ecad-75c3-4fe4-a18c-6be5f62ba8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = [\n",
    "    Row(book_id=1, title=\"Book A\", author_fname=\"John\", author_lname=\"Doe\", pages=300, released_year=2005, stock_quantity=55),\n",
    "    Row(book_id=2, title=\"Book B\", author_fname=\"Jane\", author_lname=\"Smith\", pages=250, released_year=2010, stock_quantity=40),\n",
    "    Row(book_id=3, title=\"Book C\", author_fname=\"Emily\", author_lname=\"Jones\", pages=180, released_year=2015, stock_quantity=20),\n",
    "    Row(book_id=4, title=\"Book D\", author_fname=\"Chris\", author_lname=\"Brown\", pages=320, released_year=2012, stock_quantity=75),\n",
    "    Row(book_id=5, title=\"Book E\", author_fname=\"Anna\", author_lname=\"Davis\", pages=270, released_year=2008, stock_quantity=35)\n",
    "]\n",
    "\n",
    "books_df = spark.createDataFrame(books_data)\n",
    "books_df.createOrReplaceTempView(\"books\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41904fb3-68a8-4ae2-8ff1-0fa6583b3840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|username|  adress|\n",
      "+--------+--------+\n",
      "|       A|    서울|\n",
      "|       B|    대전|\n",
      "|       C|  경기도|\n",
      "|       D|주소없음|\n",
      "|       E|주소없음|\n",
      "|       F|    부산|\n",
      "|       G|    대구|\n",
      "|       H|    광주|\n",
      "|       I|    울산|\n",
      "|       J|  강원도|\n",
      "|       K|  충청도|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 두번째 쿼리\n",
    "query_users = '''\n",
    "SELECT username,\n",
    "\tIF(address IS NULL, '주소없음', address) AS adress\n",
    "from users;\n",
    "'''\n",
    "spark.sql(query_users).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52a6168-25ee-4290-ad0b-3353ae2caf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|address|region|\n",
      "+-------+------+\n",
      "|   서울|수도권|\n",
      "|   대전|  지방|\n",
      "| 경기도|수도권|\n",
      "|   null|  지방|\n",
      "|   null|  지방|\n",
      "|   부산|  지방|\n",
      "|   대구|  지방|\n",
      "|   광주|  지방|\n",
      "|   울산|  지방|\n",
      "| 강원도|  지방|\n",
      "| 충청도|  지방|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 세번쨰 쿼리\n",
    "query_users = '''\n",
    "SELECT address,\n",
    "       IF(address IN ('경기도', '서울'), '수도권', '지방') AS region\n",
    "FROM users;\n",
    "'''\n",
    "spark.sql(query_users).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b7ba04e-1f85-435d-a3a3-3c0022094d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+------------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|stock_status|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+------------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            55|    재고많음|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            40|        중간|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            20|    재고없음|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            75|    재고많음|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            35|        중간|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 네번쨰 쿼리, books table\n",
    "# stock_quantity >= 50 '재고많음', >=30 '중간', '재고없음'\n",
    "query_users_1 = '''\n",
    "SELECT *,\n",
    "       CASE \n",
    "           WHEN stock_quantity >= 50 THEN '재고많음'\n",
    "           WHEN stock_quantity >= 30 THEN '재고중간'\n",
    "           ELSE '재고없음'\n",
    "       END AS stock_status\n",
    "FROM books;\n",
    "'''\n",
    "spark.sql(query_users).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d47a9a9-5f70-42fd-b0c8-ccb770b7808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [book_id#6L, title#7, author_fname#8, author_lname#9, pages#10L, released_year#11L, stock_quantity#12L, CASE WHEN (stock_quantity#12L >= 50) THEN 재고많음 WHEN (stock_quantity#12L >= 30) THEN 재고중간 ELSE 재고없음 END AS stock_status#198]\n",
      "+- *(1) Scan ExistingRDD[book_id#6L,title#7,author_fname#8,author_lname#9,pages#10L,released_year#11L,stock_quantity#12L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_users_1).explain() #IF문을 써서 할때랑 차이가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0426aac-fc34-4318-b546-76535bbaf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[author_lname#9], functions=[])\n",
      "+- Exchange hashpartitioning(author_lname#9, 200), ENSURE_REQUIREMENTS, [id=#115]\n",
      "   +- *(1) HashAggregate(keys=[author_lname#9], functions=[])\n",
      "      +- *(1) Project [author_lname#9]\n",
      "         +- *(1) Scan ExistingRDD[book_id#6L,title#7,author_fname#8,author_lname#9,pages#10L,released_year#11L,stock_quantity#12L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_2 = '''\n",
    "select distinct author_lname from books\n",
    "'''\n",
    "spark.sql(books_sql_2).explain() #IF문을 써서 할때랑 차이가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcd5c2a4-6ada-4b13-aa5d-9ee41ba2d92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|author_lname|\n",
      "+------------+\n",
      "|       Jones|\n",
      "|       Davis|\n",
      "|       Smith|\n",
      "|         Doe|\n",
      "|       Brown|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(books_sql_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a509ada7-cac2-4c1c-9396-edada24fc146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[author_lname#9], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(author_lname#9, 200), ENSURE_REQUIREMENTS, [id=#183]\n",
      "   +- *(1) HashAggregate(keys=[author_lname#9], functions=[partial_count(1)])\n",
      "      +- *(1) Project [author_lname#9]\n",
      "         +- *(1) Scan ExistingRDD[book_id#6L,title#7,author_fname#8,author_lname#9,pages#10L,released_year#11L,stock_quantity#12L]\n",
      "\n",
      "\n",
      "+------------+--------+\n",
      "|author_lname|count(1)|\n",
      "+------------+--------+\n",
      "|       Jones|       1|\n",
      "|       Davis|       1|\n",
      "|       Smith|       1|\n",
      "|         Doe|       1|\n",
      "|       Brown|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_3 = '''\n",
    "SELECT author_lname, COUNT(*)\n",
    "FROM books\n",
    "GROUP BY author_lname;\n",
    "'''\n",
    "\n",
    "spark.sql(books_sql_3).explain()\n",
    "spark.sql(books_sql_3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10992814-dffe-4499-87ce-83d9786ca51e",
   "metadata": {},
   "source": [
    "## 데이터 변경(조인을 위해 연결성 부여)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9f7425-4257-41f8-b651-47b3f4d39955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# books 테이블 데이터에 borrowed_by 추가\n",
    "books_data_with_user = [\n",
    "    Row(book_id=1, title=\"Book A\", author_fname=\"John\", author_lname=\"Doe\", pages=300, released_year=2005, stock_quantity=55, borrowed_by=1),\n",
    "    Row(book_id=2, title=\"Book B\", author_fname=\"Jane\", author_lname=\"Smith\", pages=250, released_year=2010, stock_quantity=40, borrowed_by=2),\n",
    "    Row(book_id=3, title=\"Book C\", author_fname=\"Emily\", author_lname=\"Jones\", pages=180, released_year=2015, stock_quantity=20, borrowed_by=3),\n",
    "    Row(book_id=4, title=\"Book D\", author_fname=\"Chris\", author_lname=\"Brown\", pages=320, released_year=2012, stock_quantity=75, borrowed_by=None),\n",
    "    Row(book_id=5, title=\"Book E\", author_fname=\"Anna\", author_lname=\"Davis\", pages=270, released_year=2008, stock_quantity=35, borrowed_by=6)\n",
    "]\n",
    "\n",
    "# DataFrame 생성\n",
    "books_df_with_user = spark.createDataFrame(books_data_with_user)\n",
    "\n",
    "# Temp View 등록\n",
    "books_df_with_user.createOrReplaceTempView(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7eaa780-6d80-4607-ac11-0e0abaf9005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            55|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            40|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            20|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            75|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            35|          6|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_3 = '''\n",
    "SELECT *\n",
    "FROM books\n",
    "'''\n",
    "\n",
    "spark.sql(books_sql_3).explain()\n",
    "spark.sql(books_sql_3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad85683a-255c-478a-bc81-4ed058a30397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            55|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            40|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            50|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            75|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            35|          6|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# book_id=3, stock_quantity=50으로 바꾼다. > 데이터 전처리 과정\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "updated_books_df = books_df_with_user.withColumn(\n",
    "    \"stock_quantity\",\n",
    "    when(books_df_with_user.book_id == 3, 50).otherwise(books_df_with_user.stock_quantity)\n",
    ")\n",
    "\n",
    "updated_books_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c63274ec-0174-4c7c-8249-693b0c041051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            60|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            44|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            22|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            82|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            38|          6|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stock_quantity 값을 10% 증가\n",
    "updated_books_df = books_df_with_user.withColumn(\n",
    "    \"stock_quantity\",\n",
    "    (col(\"stock_quantity\") * 1.1).cast(\"int\")  # 10% 증가 후 정수로 변환\n",
    ")\n",
    "# 뷰로 등록\n",
    "updated_books_df.createOrReplaceTempView(\"updated_books\")\n",
    "\n",
    "# 확인\n",
    "updated_books_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f56dbd5-327c-467f-b5c8-8f836027b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 저장 : overwrite, append, ignore, error\n",
    "updated_books_df.write.csv(\"data/output/sqltest_updated_books.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9b72c5a-2ca3-4b39-a55e-43499cc85c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.write.csv(\"data/output/sqltest_updated_users.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bdcb238-706f-4d51-afbe-3f147e8a9720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            22|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            82|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            38|          6|\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            60|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            44|          2|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_books_df1 = spark.read.csv(\"data/output/sqltest_updated_books.csv\", header = True)\n",
    "updated_books_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b5f8363-3b54-450f-b856-cc7ea48e6a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|      5|Book E|        Anna|       Davis|       F|   부산|\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "|      3|Book C|       Emily|       Jones|       C| 경기도|\n",
      "|      2|Book B|        Jane|       Smith|       B|   대전|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 조인 실습\n",
    "# book_id, title, author_fname, author_lname, username, address\n",
    "join_query = '''\n",
    "SELECT book_id, title, author_fname, author_lname, username, address\n",
    "FROM books b INNER JOIN users u ON b.borrowed_by = u.user_id; \n",
    "'''\n",
    "\n",
    "spark.sql(join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6908440-8d81-4184-9e23-352a42de9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71bf9ca5-1fe6-46b5-8c77-8606a0e9370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|   null|  null|        null|        null|       G|   대구|\n",
      "|      5|Book E|        Anna|       Davis|       F|   부산|\n",
      "|   null|  null|        null|        null|       I|   울산|\n",
      "|   null|  null|        null|        null|       E|   null|\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "|   null|  null|        null|        null|       J| 강원도|\n",
      "|      3|Book C|       Emily|       Jones|       C| 경기도|\n",
      "|   null|  null|        null|        null|       H|   광주|\n",
      "|   null|  null|        null|        null|       K| 충청도|\n",
      "|      2|Book B|        Jane|       Smith|       B|   대전|\n",
      "|   null|  null|        null|        null|       D|   null|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자의 책 대여 목록 > 전체 사용자 > 대여한 정보가 있으면 나오고 없으면 NULL\n",
    "# RIGHT JOIN\n",
    "\n",
    "join_query = '''\n",
    "SELECT book_id, title, author_fname, author_lname, username, address\n",
    "FROM books b RIGHT JOIN users u ON b.borrowed_by = u.user_id; \n",
    "'''\n",
    "\n",
    "spark.sql(join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cf71f49-ef55-4a31-92ba-58fd7ba50c71",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [book_id#241L, title#242, author_fname#243, author_lname#244, username#1, address#2]\n",
      "+- SortMergeJoin [borrowed_by#248L], [user_id#0L], RightOuter\n",
      "   :- *(2) Sort [borrowed_by#248L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(borrowed_by#248L, 200), ENSURE_REQUIREMENTS, [id=#614]\n",
      "   :     +- *(1) Project [book_id#241L, title#242, author_fname#243, author_lname#244, borrowed_by#248L]\n",
      "   :        +- *(1) Filter isnotnull(borrowed_by#248L)\n",
      "   :           +- *(1) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "   +- *(4) Sort [user_id#0L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(user_id#0L, 200), ENSURE_REQUIREMENTS, [id=#619]\n",
      "         +- *(3) Filter (isnotnull(address#2) AND (address#2 = 서울))\n",
      "            +- *(3) Scan ExistingRDD[user_id#0L,username#1,address#2]\n",
      "\n",
      "\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정 지역 = 서울에 거주하는 사용자가 대여한 책 목록\n",
    "query = '''\n",
    "SELECT book_id, title, author_fname, author_lname, username, address\n",
    "FROM books b RIGHT JOIN users u ON b.borrowed_by = u.user_id\n",
    "WHERE u.address = '서울';\n",
    "'''\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d058fe45-49dd-4917-8bbb-0ee7292db928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[user_id#0L, username#1], functions=[count(book_id#241L)])\n",
      "+- *(5) HashAggregate(keys=[user_id#0L, username#1], functions=[partial_count(book_id#241L)])\n",
      "   +- *(5) Project [user_id#0L, username#1, book_id#241L]\n",
      "      +- SortMergeJoin [user_id#0L], [borrowed_by#248L], LeftOuter\n",
      "         :- *(2) Sort [user_id#0L ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(user_id#0L, 200), ENSURE_REQUIREMENTS, [id=#724]\n",
      "         :     +- *(1) Project [user_id#0L, username#1]\n",
      "         :        +- *(1) Scan ExistingRDD[user_id#0L,username#1,address#2]\n",
      "         +- *(4) Sort [borrowed_by#248L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(borrowed_by#248L, 200), ENSURE_REQUIREMENTS, [id=#729]\n",
      "               +- *(3) Project [book_id#241L, borrowed_by#248L]\n",
      "                  +- *(3) Filter isnotnull(borrowed_by#248L)\n",
      "                     +- *(3) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+\n",
      "|user_id|username|count(book_id)|\n",
      "+-------+--------+--------------+\n",
      "|      7|       G|             0|\n",
      "|      6|       F|             1|\n",
      "|      9|       I|             0|\n",
      "|      5|       E|             0|\n",
      "|      1|       A|             1|\n",
      "|     10|       J|             0|\n",
      "|      3|       C|             1|\n",
      "|      8|       H|             0|\n",
      "|     11|       K|             0|\n",
      "|      2|       B|             1|\n",
      "|      4|       D|             0|\n",
      "+-------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자별로 대여한 책의 수(users가 중심이 되어야함)\n",
    "query = '''\n",
    "SELECT u.user_id, u.username, COUNT(b.book_id)\n",
    "FROM users u LEFT JOIN books b ON u.user_id = b.borrowed_by\n",
    "GROUP BY u.user_id, u.username;\n",
    "'''\n",
    "\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cce64f6f-3ff1-4ae1-a255-195c378797fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [book_id#241L, pages#245L, title#242, CASE WHEN (pages#245L >= 300) THEN Long ELSE Short END AS page_category#820]\n",
      "+- *(1) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n",
      "+-------+-----+------+-------------+\n",
      "|book_id|pages| title|page_category|\n",
      "+-------+-----+------+-------------+\n",
      "|      1|  300|Book A|         Long|\n",
      "|      2|  250|Book B|        Short|\n",
      "|      3|  180|Book C|        Short|\n",
      "|      4|  320|Book D|         Long|\n",
      "|      5|  270|Book E|        Short|\n",
      "+-------+-----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 300페이지 이상이면 long, 아니면 short으로 하는 컬럼 만들기\n",
    "query = '''\n",
    "SELECT book_id, pages, title, CASE WHEN pages>=300 THEN 'Long' ELSE 'Short' END AS page_category\n",
    "FROM books;\n",
    "'''\n",
    "\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86678f15-f252-4434-855c-2bebf923de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [book_id#241L, stock_quantity#247L, title#242, CASE WHEN (stock_quantity#247L >= 50) THEN 충분 WHEN (stock_quantity#247L >= 30) THEN 보통 ELSE 부족 END AS stock_status#847]\n",
      "+- *(1) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n",
      "+-------+--------------+------+------------+\n",
      "|book_id|stock_quantity| title|stock_status|\n",
      "+-------+--------------+------+------------+\n",
      "|      1|            55|Book A|        충분|\n",
      "|      2|            40|Book B|        보통|\n",
      "|      3|            20|Book C|        부족|\n",
      "|      4|            75|Book D|        충분|\n",
      "|      5|            35|Book E|        보통|\n",
      "+-------+--------------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stock_quantity > 50이면 충분, 30이상이면 보통, 아니면 부족\n",
    "query = '''\n",
    "SELECT book_id, stock_quantity, title, \n",
    "       CASE \n",
    "           WHEN stock_quantity >= 50 THEN '충분'\n",
    "           WHEN stock_quantity >= 30 THEN '보통'\n",
    "           ELSE '부족'\n",
    "       END AS stock_status\n",
    "FROM books;\n",
    "'''\n",
    "\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71533c8e-e569-4240-90cf-21a6c92a2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[author_fname#243, author_lname#244], functions=[count(borrowed_by#248L)])\n",
      "+- Exchange hashpartitioning(author_fname#243, author_lname#244, 200), ENSURE_REQUIREMENTS, [id=#950]\n",
      "   +- *(1) HashAggregate(keys=[author_fname#243, author_lname#244], functions=[partial_count(borrowed_by#248L)])\n",
      "      +- *(1) Project [author_fname#243, author_lname#244, borrowed_by#248L]\n",
      "         +- *(1) Filter isnotnull(borrowed_by#248L)\n",
      "            +- *(1) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n",
      "+------------+------------+------------+\n",
      "|author_fname|author_lname|borrow_count|\n",
      "+------------+------------+------------+\n",
      "|        Anna|       Davis|           1|\n",
      "|       Emily|       Jones|           1|\n",
      "|        John|         Doe|           1|\n",
      "|        Jane|       Smith|           1|\n",
      "+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 작가별 대여된 책의 수\n",
    "query = '''\n",
    "SELECT author_fname, author_lname, COUNT(borrowed_by) AS borrow_count\n",
    "FROM books\n",
    "WHERE borrowed_by IS NOT NULL\n",
    "GROUP BY author_fname, author_lname;\n",
    "'''\n",
    "\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2182f43-36dd-4771-9896-99e144700b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[released_year#246L], functions=[count(borrowed_by#248L)])\n",
      "+- Exchange hashpartitioning(released_year#246L, 200), ENSURE_REQUIREMENTS, [id=#1001]\n",
      "   +- *(1) HashAggregate(keys=[released_year#246L], functions=[partial_count(borrowed_by#248L)])\n",
      "      +- *(1) Project [released_year#246L, borrowed_by#248L]\n",
      "         +- *(1) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n",
      "+-------------+------------+\n",
      "|released_year|borrow_count|\n",
      "+-------------+------------+\n",
      "|         2012|           0|\n",
      "|         2010|           1|\n",
      "|         2005|           1|\n",
      "|         2008|           1|\n",
      "|         2015|           1|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 책의 발행 연도별 대여 현황\n",
    "query = '''\n",
    "SELECT released_year, COUNT(borrowed_by) AS borrow_count\n",
    "FROM books\n",
    "GROUP BY released_year;\n",
    "'''\n",
    "\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8235240-ac5e-4fc7-93e5-7ed3c0f37408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=1, orderBy=[pages#245L DESC NULLS LAST], output=[book_id#241L,title#242,pages#245L])\n",
      "+- *(1) Project [book_id#241L, title#242, pages#245L]\n",
      "   +- *(1) Filter isnull(borrowed_by#248L)\n",
      "      +- *(1) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n",
      "+-------+------+-----+\n",
      "|book_id| title|pages|\n",
      "+-------+------+-----+\n",
      "|      4|Book D|  320|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 대여되지 않은 책 중 페이지 수가 가장 많은 책\n",
    "query = '''\n",
    "SELECT book_id, title, pages\n",
    "FROM books\n",
    "WHERE borrowed_by IS NULL\n",
    "ORDER BY pages DESC\n",
    "LIMIT 1;\n",
    "'''\n",
    "\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dac32437-8aa6-4ae5-9360-a90731071dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[address#2], functions=[count(borrowed_by#248L)])\n",
      "+- Exchange hashpartitioning(address#2, 200), ENSURE_REQUIREMENTS, [id=#1176]\n",
      "   +- *(5) HashAggregate(keys=[address#2], functions=[partial_count(borrowed_by#248L)])\n",
      "      +- *(5) Project [address#2, borrowed_by#248L]\n",
      "         +- *(5) SortMergeJoin [user_id#0L], [borrowed_by#248L], Inner\n",
      "            :- *(2) Sort [user_id#0L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(user_id#0L, 200), ENSURE_REQUIREMENTS, [id=#1161]\n",
      "            :     +- *(1) Project [user_id#0L, address#2]\n",
      "            :        +- *(1) Filter isnotnull(user_id#0L)\n",
      "            :           +- *(1) Scan ExistingRDD[user_id#0L,username#1,address#2]\n",
      "            +- *(4) Sort [borrowed_by#248L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(borrowed_by#248L, 200), ENSURE_REQUIREMENTS, [id=#1167]\n",
      "                  +- *(3) Project [borrowed_by#248L]\n",
      "                     +- *(3) Filter isnotnull(borrowed_by#248L)\n",
      "                        +- *(3) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|address|borrow_count|\n",
      "+-------+------------+\n",
      "|   대전|           1|\n",
      "| 경기도|           1|\n",
      "|   부산|           1|\n",
      "|   서울|           1|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자의 지역별 대여된 책 수\n",
    "query = '''\n",
    "SELECT book_id, title, stock_quantity, \n",
    "       CASE \n",
    "           WHEN stock_quantity < 30 THEN '부족'\n",
    "           ELSE '충분'\n",
    "       END AS stock_status,\n",
    "       \n",
    "FROM books\n",
    "WHERE stock_quantity < 30;\n",
    "'''\n",
    "\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3bfc94e2-8eda-438d-8874-6f1f4308b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [book_id#241L, title#242, stock_quantity#247L, CASE WHEN (stock_quantity#247L < 30) THEN 부족 ELSE 충분 END AS stock_status#1010]\n",
      "+- *(1) Filter (isnotnull(stock_quantity#247L) AND (stock_quantity#247L < 30))\n",
      "   +- *(1) Scan ExistingRDD[book_id#241L,title#242,author_fname#243,author_lname#244,pages#245L,released_year#246L,stock_quantity#247L,borrowed_by#248L]\n",
      "\n",
      "\n",
      "+-------+------+--------------+------------+\n",
      "|book_id| title|stock_quantity|stock_status|\n",
      "+-------+------+--------------+------------+\n",
      "|      3|Book C|            20|        부족|\n",
      "+-------+------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 재고가 부족한 책과 대여 상태 확인\n",
    "query = '''\n",
    "SELECT book_id, title, stock_quantity, \n",
    "       CASE \n",
    "           WHEN stock_quantity < 30 THEN '부족'\n",
    "           ELSE '충분'\n",
    "       END AS stock_status\n",
    "FROM books\n",
    "WHERE stock_quantity < 30 ; -- stock_quatity를 바로 조건식으로 걸 수는 없음. 실행 순서로 인해\n",
    "'''\n",
    "\n",
    "spark.sql(query).explain()\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "826d8d8d-a137-4778-bb54-4cba3327886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c7b5e-7512-4187-b929-6f4d9dc5f090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a8f9f-af7d-4c9a-9442-937907d86379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf328382-6cc4-4898-bf31-15598e50c5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f2445-9b77-4a44-9af1-3567a6a2152e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765bd50-f20d-48ba-b251-fee9f961be5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
