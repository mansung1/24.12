{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54943918-d25a-408f-a701-1d381043ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 15:50:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"taxi\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a493c325-27d7-464f-b922-3dd5dd566d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow202101 = spark.read.format('csv').load('data/yellow_tripdata_2021-01.csv', inferSchema=True, header=True)\n",
    "yellow202102 = spark.read.format('csv').load('data/yellow_tripdata_2021-02.csv', inferSchema=True, header=True)\n",
    "yellow202103 = spark.read.format('csv').load('data/yellow_tripdata_2021-03.csv', inferSchema=True, header=True)\n",
    "yellow202104 = spark.read.format('csv').load('data/yellow_tripdata_2021-04.csv', inferSchema=True, header=True)\n",
    "yellow202105 = spark.read.format('csv').load('data/yellow_tripdata_2021-05.csv', inferSchema=True, header=True)\n",
    "yellow202106 = spark.read.format('csv').load('data/yellow_tripdata_2021-06.csv', inferSchema=True, header=True)\n",
    "yellow202107 = spark.read.format('csv').load('data/yellow_tripdata_2021-07.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1771fa0d-80fb-4afd-ae3a-b88f081b9f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_df = spark.read.format('csv').load('data/taxi+_zone_lookup.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b94d1de9-af0d-4dfc-97f8-45e7b17a8b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06cf489-e4fc-41a1-9509-68930fb50b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = yellow202101.union(yellow202102).union(yellow202103).union(yellow202104).union(yellow202105).union(yellow202106).union(yellow202107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2221e1b3-a82a-4d0c-9ddf-b83691539b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef9d7183-05f7-4fbd-9af5-7ca62388eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView('trips')\n",
    "zone_df.createOrReplaceTempView('zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe343619-b85d-4387-bdbd-c44a7746217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#분석에 필요할 컬럼들만 따로 빼서 출력\n",
    "query = '''\n",
    "select\n",
    "t.VendorID,\n",
    "TO_DATE(t.tpep_pickup_datetime) as pickup_date,\n",
    "TO_DATE(t.tpep_dropoff_datetime) as dropoff_date,\n",
    "HOUR(t.tpep_pickup_datetime)  as pickup_time,\n",
    "HOUR(t.tpep_dropoff_datetime) as dropoff_time,\n",
    "t.passenger_count,\n",
    "t.trip_distance,\n",
    "t.tip_amount,\n",
    "t.total_amount,\n",
    "t.payment_type,\n",
    "pz.Zone as pickup_zone,\n",
    "dz.Zone as dropoff_zone\n",
    "from trips t\n",
    "LEFT JOIN zone pz ON t.PULocationID = pz.LocationID\n",
    "LEFT JOIN zone dz ON t.DOLocationID = dz.LocationID\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd06aabf-ccce-4efe-827c-fed17250ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6cf623a-11e2-4e83-98d9-6ff33bd6e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15000700"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_df.count() #1500만건의 데이터..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1164e3b9-2a0f-41df-9786-cd6aa6fd9f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+--------------------+----------------+\n",
      "|VendorID|pickup_date|dropoff_date|pickup_time|dropoff_time|passenger_count|trip_distance|tip_amount|total_amount|payment_type|         pickup_zone|    dropoff_zone|\n",
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+--------------------+----------------+\n",
      "|       1| 2021-01-01|  2021-01-01|          0|           0|              1|          2.1|       0.0|        11.8|           2| Lincoln Square East|    Central Park|\n",
      "|       1| 2021-01-01|  2021-01-01|          0|           0|              1|          0.2|       0.0|         4.3|           2|Upper West Side N...|Manhattan Valley|\n",
      "|       1| 2021-01-01|  2021-01-01|          0|           1|              1|         14.7|      8.65|       51.95|           1|         JFK Airport|         Midwood|\n",
      "|       1| 2021-01-01|  2021-01-01|          0|           0|              0|         10.6|      6.05|       36.35|           1|   LaGuardia Airport|     JFK Airport|\n",
      "|       2| 2021-01-01|  2021-01-01|          0|           0|              1|         4.94|      4.06|       24.36|           1|        East Chelsea|Brooklyn Heights|\n",
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b098a30-4384-4f19-9b9e-0f9b93d89fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|pickup_time|\n",
      "+-----------+-----------+\n",
      "| 2020-12-31|         23|\n",
      "| 2020-12-31|         21|\n",
      "| 2020-12-31|         23|\n",
      "| 2020-12-31|         18|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2020-12-31|         23|\n",
      "| 2020-12-31|         13|\n",
      "| 2020-12-31|         23|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_df.createOrReplaceTempView('comb')\n",
    "\n",
    "query = '''\n",
    "select pickup_date, pickup_time \n",
    "from comb \n",
    "where pickup_time>0\n",
    "'''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "856d6fcb-99ad-4aff-9f29-2825ddfdfeb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|pickup_time|\n",
      "+-----------+-----------+\n",
      "| 2009-01-01|          0|\n",
      "| 2008-12-31|         23|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2008-12-31|         23|\n",
      "| 2008-12-31|         23|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          1|\n",
      "| 2009-01-01|         17|\n",
      "| 2009-01-01|         17|\n",
      "| 2009-01-01|         18|\n",
      "| 2009-01-01|          1|\n",
      "| 2009-01-01|          1|\n",
      "| 2009-01-01|          2|\n",
      "| 2009-01-01|          2|\n",
      "| 2009-01-01|          0|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select pickup_date, pickup_time \n",
    "from comb \n",
    "where pickup_date<'2020-12-31'\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cb209c5-24d1-4d94-bcdb-a4ce995837b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, VendorID: string, pickup_time: string, dropoff_time: string, passenger_count: string, trip_distance: string, tip_amount: string, total_amount: string, payment_type: string, pickup_zone: string, dropoff_zone: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270de08e-263e-4811-8b2c-e873c9c528f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8545c58d-5ba5-4801-b408-c5d663229c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|pickup_time|\n",
      "+-----------+-----------+\n",
      "| 2009-01-01|          0|\n",
      "| 2008-12-31|         23|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2008-12-31|         23|\n",
      "| 2008-12-31|         23|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          1|\n",
      "| 2009-01-01|         17|\n",
      "| 2009-01-01|         17|\n",
      "| 2009-01-01|         18|\n",
      "| 2009-01-01|          1|\n",
      "| 2009-01-01|          1|\n",
      "| 2009-01-01|          2|\n",
      "| 2009-01-01|          2|\n",
      "| 2009-01-01|          0|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query1 = '''\n",
    "select pickup_date, pickup_time\n",
    "from comb\n",
    "where pickup_date < '2020-12-31'\n",
    "'''\n",
    "\n",
    "spark.sql(query1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f7453a2-2c8c-431a-8bdc-06f0a3476336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|pickup_time|\n",
      "+-----------+-----------+\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "| 2021-01-01|          1|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 계획, 실행 결과(4040에서 확인해보기)\n",
    "query2 = '''\n",
    "select pickup_date, pickup_time\n",
    "from comb\n",
    "where pickup_time > 0 and pickup_time<=12\n",
    "'''\n",
    "\n",
    "spark.sql(query2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09d2502d-1b24-449c-95e0-5cff24f97ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(12) Sort [pickup_date#992 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(pickup_date#992 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2535]\n",
      "   +- *(11) HashAggregate(keys=[pickup_date#992], functions=[first(pickup_time#994, false)])\n",
      "      +- Exchange hashpartitioning(pickup_date#992, 200), ENSURE_REQUIREMENTS, [id=#2531]\n",
      "         +- *(10) HashAggregate(keys=[pickup_date#992], functions=[partial_first(pickup_time#994, false)])\n",
      "            +- *(10) Project [cast(tpep_pickup_datetime#41 as date) AS pickup_date#992, hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/Seoul)) AS pickup_time#994]\n",
      "               +- *(10) BroadcastHashJoin [DOLocationID#48], [LocationID#998], LeftOuter, BuildRight, false\n",
      "                  :- *(10) Project [tpep_pickup_datetime#41, DOLocationID#48]\n",
      "                  :  +- *(10) BroadcastHashJoin [PULocationID#47], [LocationID#666], LeftOuter, BuildRight, false\n",
      "                  :     :- Union\n",
      "                  :     :  :- *(1) Filter (isnotnull(tpep_pickup_datetime#41) AND (hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :  :  +- FileScan csv [tpep_pickup_datetime#41,PULocationID#47,DOLocationID#48] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#41), (hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab02/src/data/yellow_tripdata_2021-01.csv], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     :  :- *(2) Filter (isnotnull(tpep_pickup_datetime#93) AND (hour(cast(tpep_pickup_datetime#93 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :  :  +- FileScan csv [tpep_pickup_datetime#93,PULocationID#99,DOLocationID#100] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#93), (hour(cast(tpep_pickup_datetime#93 as timestamp), Some(Asia/..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab02/src/data/yellow_tripdata_2021-02.csv], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     :  :- *(3) Filter (isnotnull(tpep_pickup_datetime#145) AND (hour(cast(tpep_pickup_datetime#145 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :  :  +- FileScan csv [tpep_pickup_datetime#145,PULocationID#151,DOLocationID#152] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#145), (hour(cast(tpep_pickup_datetime#145 as timestamp), Some(Asi..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab02/src/data/yellow_tripdata_2021-03.csv], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     :  :- *(4) Filter (isnotnull(tpep_pickup_datetime#197) AND (hour(cast(tpep_pickup_datetime#197 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :  :  +- FileScan csv [tpep_pickup_datetime#197,PULocationID#203,DOLocationID#204] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#197), (hour(cast(tpep_pickup_datetime#197 as timestamp), Some(Asi..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab02/src/data/yellow_tripdata_2021-04.csv], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     :  :- *(5) Filter (isnotnull(tpep_pickup_datetime#249) AND (hour(cast(tpep_pickup_datetime#249 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :  :  +- FileScan csv [tpep_pickup_datetime#249,PULocationID#255,DOLocationID#256] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#249), (hour(cast(tpep_pickup_datetime#249 as timestamp), Some(Asi..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab02/src/data/yellow_tripdata_2021-05.csv], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     :  :- *(6) Filter (isnotnull(tpep_pickup_datetime#301) AND (hour(cast(tpep_pickup_datetime#301 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :  :  +- FileScan csv [tpep_pickup_datetime#301,PULocationID#307,DOLocationID#308] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#301), (hour(cast(tpep_pickup_datetime#301 as timestamp), Some(Asi..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab02/src/data/yellow_tripdata_2021-06.csv], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     :  +- *(7) Filter (isnotnull(tpep_pickup_datetime#353) AND (hour(cast(tpep_pickup_datetime#353 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :     +- FileScan csv [tpep_pickup_datetime#353,PULocationID#359,DOLocationID#360] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#353), (hour(cast(tpep_pickup_datetime#353 as timestamp), Some(Asi..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab02/src/data/yellow_tripdata_2021-07.csv], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#2492]\n",
      "                  :        +- *(8) Filter isnotnull(LocationID#666)\n",
      "                  :           +- FileScan csv [LocationID#666] Batched: false, DataFilters: [isnotnull(LocationID#666)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab02/src/data/taxi+_zone_lookup.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int>\n",
      "                  +- ReusedExchange [LocationID#998], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#2492]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = '''\n",
    "select \n",
    "    pickup_date, \n",
    "    first(pickup_time) as pickup_time\n",
    "from comb\n",
    "where pickup_time > 0\n",
    "group by pickup_date\n",
    "order by pickup_date\n",
    "'''\n",
    "\n",
    "spark.sql(query3).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71bd22-71b6-4988-9058-99a411ca5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c07604-0410-48df-ab35-c252c16c83b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c15f85-92af-440b-8522-839bdead7283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf54f57-4529-450f-b0e8-1e8ab4b0d8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095aeb27-eac0-459b-a371-fe87163a37a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee914c7-38f9-47b3-a830-bfd759400d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b6a80-8765-471a-b0e3-1bbe23b21bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
